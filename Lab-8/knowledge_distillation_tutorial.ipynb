{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "pKoPveSma__f"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TeacherModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TeacherModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 1200)\n",
        "        self.fc2 = nn.Linear(1200, 1200)\n",
        "        self.fc3 = nn.Linear(1200, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "k4BH5HI2bCr1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StudentModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StudentModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 800)\n",
        "        self.fc2 = nn.Linear(800, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "4GhhkMWhbEpP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzArprHybIFu",
        "outputId": "40ec1698-87c2-46dc-8f32-9999f49579f1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.51MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 57.7kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.08MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.57MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def distillation_loss(student_logits, teacher_logits, labels, temperature, alpha):\n",
        "    soft_teacher = F.softmax(teacher_logits / temperature, dim=1)\n",
        "    soft_student = F.log_softmax(student_logits / temperature, dim=1)\n",
        "    kl_div = F.kl_div(soft_student, soft_teacher) * (temperature ** 2)\n",
        "    ce_loss = F.cross_entropy(student_logits, labels)\n",
        "    return alpha * kl_div + (1 - alpha) * ce_loss"
      ],
      "metadata": {
        "id": "pyvMKTYHbKZJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher = TeacherModel()\n",
        "optimizer = optim.Adam(teacher.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "ihjNAuYHbOuQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_teacher(model, train_loader, optimizer, criterion, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for data, target in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Teacher Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "train_teacher(teacher, train_loader, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXiVzTkbbm9K",
        "outputId": "8ec333fe-8251-4d32-a92f-a2bf6115fd6a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher Epoch 1/5, Loss: 0.06031704694032669\n",
            "Teacher Epoch 2/5, Loss: 0.20478422939777374\n",
            "Teacher Epoch 3/5, Loss: 0.005286639556288719\n",
            "Teacher Epoch 4/5, Loss: 0.10368572175502777\n",
            "Teacher Epoch 5/5, Loss: 0.07845836132764816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student = StudentModel()\n",
        "optimizer = optim.Adam(student.parameters(), lr=0.001)\n",
        "\n",
        "def train_student(student, teacher, train_loader, optimizer, temperature=5.0, alpha=0.7, epochs=5):\n",
        "    student.train()\n",
        "    teacher.eval()\n",
        "    for epoch in range(epochs):\n",
        "        for data, target in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            student_logits = student(data)\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = teacher(data)\n",
        "            loss = distillation_loss(student_logits, teacher_logits, target, temperature, alpha)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Student Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "train_student(student, teacher, train_loader, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWDM0KZCbvYJ",
        "outputId": "ece803f9-045f-46bc-9c93-f261bff13395"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student Epoch 1/5, Loss: 0.3400781750679016\n",
            "Student Epoch 2/5, Loss: 0.0622226744890213\n",
            "Student Epoch 3/5, Loss: 0.04383956640958786\n",
            "Student Epoch 4/5, Loss: 0.04491707682609558\n",
            "Student Epoch 5/5, Loss: 0.084464430809021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())"
      ],
      "metadata": {
        "id": "WtVLugy9cbrp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_params = count_parameters(teacher)\n",
        "student_params = count_parameters(student)\n",
        "\n",
        "teacher_accuracy = evaluate(teacher, test_loader)\n",
        "student_accuracy = evaluate(student, test_loader)\n",
        "\n",
        "print(f\"Teacher Model Parameters: {teacher_params}\")\n",
        "print(f\"Student Model Parameters: {student_params}\")\n",
        "\n",
        "print(f\"Teacher Accuracy: {teacher_accuracy:.2f}%, Parameters: {teacher_params}\")\n",
        "print(f\"Student Accuracy: {student_accuracy:.2f}%, Parameters: {student_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEpCrERMcdao",
        "outputId": "98415311-509b-420f-e4e7-2bdd251a5eb3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher Model Parameters: 2395210\n",
            "Student Model Parameters: 636010\n",
            "Teacher Accuracy: 97.46%, Parameters: 2395210\n",
            "Student Accuracy: 97.62%, Parameters: 636010\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}